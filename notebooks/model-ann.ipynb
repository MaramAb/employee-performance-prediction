{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import loguniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\data\\processed\\HRDataset_p_v4.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'GenderID',\n",
    "    'FromDiversityJobFairID',\n",
    "    'State',\n",
    "    'CitizenDesc',\n",
    "    'HispanicLatino',\n",
    "    'RaceDesc',\n",
    "    'Department',\n",
    "    'ManagerName',\n",
    "    'RecruitmentSource',\n",
    "    'HireYear',\n",
    "    'HireMonth'\n",
    "]\n",
    "numeric_features = [\n",
    "    'Salary',\n",
    "    'EngagementSurvey',\n",
    "    'EmpSatisfaction',\n",
    "    'SpecialProjectsCount',\n",
    "    'DaysLateLast30',\n",
    "    'Absences',\n",
    "    'Age',\n",
    "    'NumberOfColleagues'\n",
    "]\n",
    "label = 'PerfScoreID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_features + numeric_features].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = df[categorical_features + numeric_features]\n",
    "y = df[label]\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode and Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),  # we don't have missing numerical values\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MLPClassifier(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MLPClassifier(hidden_layer_sizes=(100,), activation='relu', alpha=0.0001,\n",
    "                                 max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "custom_model.fit(X_train, y_train)\n",
    "y_pred_custom = custom_model.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "precision_custom = precision_score(y_test, y_pred_custom, average='weighted')\n",
    "recall_custom = recall_score(y_test, y_pred_custom, average='weighted')\n",
    "f1_custom = f1_score(y_test, y_pred_custom, average='weighted')\n",
    "\n",
    "print(f'Baseline Accuracy: {accuracy_custom:.4f}')\n",
    "print(f'Baseline Precision: {precision_custom:.4f}')\n",
    "print(f'Baseline Recall: {recall_custom:.4f}')\n",
    "print(f'Baseline F1 Score: {f1_custom:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model without optimization\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
    "print(f'Baseline Accuracy: {accuracy:.4f}')\n",
    "print(f'Baseline Precision: {precision:.4f}')\n",
    "print(f'Baseline Recall: {recall:.4f}')\n",
    "print(f'Baseline F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search parameters\n",
    "param_grid = {\n",
    "    'classifier__hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'classifier__activation': ['relu'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=stratified_kfold, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "y_pred_grid = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate Grid Search performance\n",
    "accuracy_grid = accuracy_score(y_test, y_pred_grid)\n",
    "precision_grid = precision_score(y_test, y_pred_grid, average='weighted')\n",
    "recall_grid = recall_score(y_test, y_pred_grid, average='weighted')\n",
    "f1_grid = f1_score(y_test, y_pred_grid, average='weighted')\n",
    "\n",
    "print(f'Grid Search Accuracy: {accuracy_grid:.4f}')\n",
    "print(f'Grid Search Precision: {precision_grid:.4f}')\n",
    "print(f'Grid Search Recall: {recall_grid:.4f}')\n",
    "print(f'Grid Search F1 Score: {f1_grid:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Search Optimization:\")\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_grid)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search parameters\n",
    "param_dist = {\n",
    "    'classifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (32, 8)],\n",
    "    'classifier__activation': ['relu', 'tanh'],\n",
    "    'classifier__alpha': loguniform(1e-4, 1e-1),\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_dist, n_iter=50, cv=stratified_kfold,\n",
    "    scoring='accuracy', random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "\n",
    "# Evaluate Random Search performance\n",
    "accuracy_random = accuracy_score(y_test, y_pred_random)\n",
    "precision_random = precision_score(y_test, y_pred_random, average='weighted')\n",
    "recall_random = recall_score(y_test, y_pred_random, average='weighted')\n",
    "f1_random = f1_score(y_test, y_pred_random, average='weighted')\n",
    "\n",
    "print(f'Random Search Accuracy: {accuracy_random:.4f}')\n",
    "print(f'Random Search Precision: {precision_random:.4f}')\n",
    "print(f'Random Search Recall: {recall_random:.4f}')\n",
    "print(f'Random Search F1 Score: {f1_random:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Search Optimization:\")\n",
    "print(f'Best Parameters: {random_search.best_params_}')\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_random)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50), (100, 50), (100, 100)])\n",
    "    activation = trial.suggest_categorical('activation', ['tanh', 'relu'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-6, 1e-2)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'adaptive'])\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 3000)\n",
    "    \n",
    "    # Update the MLP model with suggested parameters\n",
    "    classifier = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create the updated pipeline\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model using accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create the study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f'Best parameters: {study.best_params}')\n",
    "print(f'Best accuracy: {study.best_value}')\n",
    "\n",
    "# Retrain the model with the best parameters\n",
    "best_params = study.best_params\n",
    "best_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "    activation=best_params['activation'],\n",
    "    solver=best_params['solver'],\n",
    "    alpha=best_params['alpha'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    random_state=42\n",
    ")\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', best_classifier)\n",
    "])\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_optuna = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Optuna optimized model\n",
    "optuna_accuracy = accuracy_score(y_test, y_pred_optuna)\n",
    "optuna_precision = precision_score(y_test, y_pred_optuna, average='weighted')\n",
    "optuna_recall = recall_score(y_test, y_pred_optuna, average='weighted')\n",
    "optuna_f1 = f1_score(y_test, y_pred_optuna, average='weighted')\n",
    "\n",
    "print(f'Optuna Optimization Accuracy: {optuna_accuracy:.4f}')\n",
    "print(f'Optuna Optimization Precision: {optuna_precision:.4f}')\n",
    "print(f'Optuna Optimization Recall: {optuna_recall:.4f}')\n",
    "print(f'Optuna Optimization F1 Score: {optuna_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Method': ['Baseline', 'Grid Search', 'Random Search', 'Optuna'],\n",
    "    'Accuracy': [accuracy, accuracy_grid, accuracy_random, optuna_accuracy],\n",
    "    'Precision': [precision, precision_grid, precision_random, optuna_precision],\n",
    "    'Recall': [recall, recall_grid, recall_random, optuna_recall],\n",
    "    'F1 Score': [f1, f1_grid, f1_random, optuna_f1]\n",
    "})\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
