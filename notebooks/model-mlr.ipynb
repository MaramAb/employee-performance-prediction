{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLR Modeling\n",
    "Multinomial Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from scipy.stats import loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/HRDataset_p_v4.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'GenderID',\n",
    "    'FromDiversityJobFairID',\n",
    "    'State',\n",
    "    'CitizenDesc',\n",
    "    'HispanicLatino',\n",
    "    'RaceDesc',\n",
    "    'Department',\n",
    "    'ManagerName',\n",
    "    'RecruitmentSource',\n",
    "    'HireYear',\n",
    "    'HireMonth'\n",
    "]\n",
    "numeric_features = [\n",
    "    'Salary',\n",
    "    'EngagementSurvey',\n",
    "    'EmpSatisfaction',\n",
    "    'SpecialProjectsCount',\n",
    "    'DaysLateLast30',\n",
    "    'Absences',\n",
    "    'Age',\n",
    "    'NumberOfColleagues'\n",
    "]\n",
    "label = 'PerfScoreID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_features + numeric_features].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = df[categorical_features + numeric_features]\n",
    "y = df[label]\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # stratify=y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode and Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),  # we don't have missing numerical values\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLR Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', max_iter=1000)) # multi_class='multinomial',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Optimization  (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model without optimization\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Grid Search Accuracy: {accuracy:.4f}')\n",
    "print(f'Grid Search Precision: {precision:.4f}')\n",
    "print(f'Grid Search Recall: {recall:.4f}')\n",
    "print(f'Grid Search F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search parameters\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__penalty': ['l2']  # 'l2' regularization\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "y_pred_grid = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate Grid Search performance\n",
    "accuracy_grid = accuracy_score(y_test, y_pred_grid)\n",
    "precision_grid = precision_score(y_test, y_pred_grid, average='weighted')\n",
    "recall_grid = recall_score(y_test, y_pred_grid, average='weighted')\n",
    "f1_grid = f1_score(y_test, y_pred_grid, average='weighted')\n",
    "\n",
    "print(\"Grid Search Optimization:\")\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_grid)}\\n\")\n",
    "print(f'Grid Search Accuracy: {accuracy_grid:.4f}')\n",
    "print(f'Grid Search Precision: {precision_grid:.4f}')\n",
    "print(f'Grid Search Recall: {recall_grid:.4f}')\n",
    "print(f'Grid Search F1 Score: {f1_grid:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search parameters\n",
    "param_dist = {\n",
    "    'classifier__C': loguniform(1e-3, 1e3),\n",
    "    'classifier__penalty': ['l2']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "\n",
    "# Evaluate Random Search performance\n",
    "accuracy_random = accuracy_score(y_test, y_pred_random)\n",
    "precision_random = precision_score(y_test, y_pred_random, average='weighted')\n",
    "recall_random = recall_score(y_test, y_pred_random, average='weighted')\n",
    "f1_random = f1_score(y_test, y_pred_random, average='weighted')\n",
    "\n",
    "print(\"Random Search Optimization:\")\n",
    "print(f'Best Parameters: {random_search.best_params_}')\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_random)}\\n\")\n",
    "print(f'Random Search Accuracy: {accuracy_random:.4f}')\n",
    "print(f'Random Search Precision: {precision_random:.4f}')\n",
    "print(f'Random Search Recall: {recall_random:.4f}')\n",
    "print(f'Random Search F1 Score: {f1_random:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization (using `skopt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter space for Bayesian optimization\n",
    "param_space = {\n",
    "    'classifier__C': (1e-6, 1e+6, 'log-uniform'),\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__solver': ['lbfgs'],\n",
    "    'classifier__max_iter': (100, 3000),\n",
    "    'classifier__multi_class': ['multinomial'],\n",
    "}\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set up the Bayesian search\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=32,  # Number of parameter settings that are sampled\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy',  # Metric to optimize\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the Bayesian search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_bayes = bayes_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Evaluate the Bayesian optimized model\n",
    "bayes_accuracy = accuracy_score(y_test, y_pred_bayes)\n",
    "bayes_precision = precision_score(y_test, y_pred_bayes, average='weighted')\n",
    "bayes_recall = recall_score(y_test, y_pred_bayes, average='weighted')\n",
    "bayes_f1 = f1_score(y_test, y_pred_bayes, average='weighted')\n",
    "\n",
    "print(f'Bayesian Optimization Accuracy: {bayes_accuracy:.4f}')\n",
    "print(f'Bayesian Optimization Precision: {bayes_precision:.4f}')\n",
    "print(f'Bayesian Optimization Recall: {bayes_recall:.4f}')\n",
    "print(f'Bayesian Optimization F1 Score: {bayes_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters\n",
    "    C = trial.suggest_loguniform('C', 1e-6, 1e6)\n",
    "    max_iter = trial.suggest_int('max_iter', 100, 3000)\n",
    "    \n",
    "    # Update the logistic regression model with suggested parameters\n",
    "    classifier = LogisticRegression(\n",
    "        C=C,\n",
    "        max_iter=max_iter,\n",
    "        solver='lbfgs',\n",
    "        multi_class='multinomial',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create the updated pipeline\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model using accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create the study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f'Best parameters: {study.best_params}')\n",
    "print(f'Best accuracy: {study.best_value}')\n",
    "\n",
    "# Retrain the model with the best parameters\n",
    "best_params = study.best_params\n",
    "best_classifier = LogisticRegression(\n",
    "    C=best_params['C'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    solver='lbfgs',\n",
    "    multi_class='multinomial',\n",
    "    random_state=42\n",
    ")\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', best_classifier)\n",
    "])\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_optuna = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Optuna optimized model\n",
    "optuna_accuracy = accuracy_score(y_test, y_pred_optuna)\n",
    "optuna_precision = precision_score(y_test, y_pred_optuna, average='weighted')\n",
    "optuna_recall = recall_score(y_test, y_pred_optuna, average='weighted')\n",
    "optuna_f1 = f1_score(y_test, y_pred_optuna, average='weighted')\n",
    "\n",
    "print(f'Optuna Optimization Accuracy: {optuna_accuracy:.4f}')\n",
    "print(f'Optuna Optimization Precision: {optuna_precision:.4f}')\n",
    "print(f'Optuna Optimization Recall: {optuna_recall:.4f}')\n",
    "print(f'Optuna Optimization F1 Score: {optuna_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Method': ['Baseline', 'Grid Search', 'Random Search', 'Optuna'],\n",
    "    'Accuracy': [accuracy, accuracy_grid, accuracy_random, optuna_accuracy],\n",
    "    'Precision': [precision, precision_grid, precision_random, optuna_precision],\n",
    "    'Recall': [recall, recall_grid, recall_random, optuna_recall],\n",
    "    'F1 Score': [f1, f1_grid, f1_random, optuna_f1]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
